{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189c91dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 21:09:49.959953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-23 21:09:49.959982: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff280dda",
   "metadata": {},
   "source": [
    "#### 1. Load the model\n",
    "\n",
    "We'll load the model and tokenizer from a saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e3a15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM (object):\n",
    "    def __init__ (self, model_checkpoint):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deae3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LM (\"../checkpoints/contextual-word-embeddings/checkpoint-9000/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0273f",
   "metadata": {},
   "source": [
    "#### 2. Forward pass\n",
    "\n",
    "- Split all the text into chunks of 512 tokens. \n",
    "- Run the forward method on each 512 token chunk. \n",
    "- For every token in a chunk get a 768*4 token representation from the final four layers.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ed71725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8edfd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2chunks (encoded_input, split_len=510):\n",
    "    # Break into smaller chunks\n",
    "    input_ids_chunks = list(encoded_input['input_ids'][0].split(split_len))\n",
    "    mask_chunks = list(encoded_input['attention_mask'][0].split(split_len))\n",
    "    \n",
    "    for i in range (len (input_ids_chunks)):\n",
    "        pad_len = 510 - input_ids_chunks[i].shape[0]\n",
    "        # check if tensor length satisfies required chunk size\n",
    "        if pad_len > 0:\n",
    "            # if padding length is more than 0, we must add padding\n",
    "            input_ids_chunks[i] = torch.cat([\n",
    "                input_ids_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "            mask_chunks[i] = torch.cat([\n",
    "                mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "            ])\n",
    "        # Append the CLS token (id=101) and the SEP token (id=102)\n",
    "        input_ids_chunks[i] = torch.cat([\n",
    "            torch.Tensor([101]), input_ids_chunks[i], torch.Tensor ([102])\n",
    "        ])\n",
    "            \n",
    "        # Add attention masks\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            torch.Tensor([1]), mask_chunks[i], torch.Tensor([1])\n",
    "        ])\n",
    "        \n",
    "    # Now aggregate into one example\n",
    "    input_ids = torch.stack(input_ids_chunks)\n",
    "    attention_mask = torch.stack(mask_chunks)\n",
    "        \n",
    "    input_dict = {\n",
    "        'input_ids': input_ids.long().clone().detach(), #torch.tensor(input_ids.long()),\n",
    "        'attention_mask': attention_mask.int().clone().detach() #torch.tensor(attention_mask.int())\n",
    "    }\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6f993c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flattened_embeddings (outputs, attention_mask):\n",
    "    # Let's concatenate the representation of the final four layers\n",
    "    embeddings = torch.cat((outputs.hidden_states[-1], #12th hidden layer\n",
    "                            outputs.hidden_states[-2], #11th hidden layer\n",
    "                            outputs.hidden_states[-3], #10th hidden layer\n",
    "                            outputs.hidden_states[-4]), dim=2)\n",
    "    embeddings = torch.flatten (embeddings[:,1:-1,:], start_dim=0, end_dim=1)\n",
    "    num_nonzero = (attention_mask[:,1:-1].flatten() == 0).nonzero(as_tuple=True)[0].size()[0]\n",
    "    if num_nonzero == 0:\n",
    "        index = None\n",
    "    else:\n",
    "        index = (attention_mask[:,1:-1].flatten() == 0).nonzero(as_tuple=True)[0][0].item()\n",
    "    return embeddings[0:index, :]\n",
    "\n",
    "def tokens_generator (toks):\n",
    "    last_token = \"\"\n",
    "    i = 0\n",
    "    token_start = 0\n",
    "    while i < len (toks):\n",
    "        if i == 0:\n",
    "            last_token = toks[i]\n",
    "            token_start = i\n",
    "        elif toks[i].startswith (\"##\"):\n",
    "            last_token = last_token + toks[i][2:]\n",
    "        else:\n",
    "            yield token_start, i, last_token\n",
    "            last_token = toks[i]\n",
    "            token_start = i\n",
    "        i += 1\n",
    "    if len (last_token) > 0:\n",
    "        yield token_start, i, last_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "8793d9f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16016/3604965945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit2chunks\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flattened_embeddings\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mwordpieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordpieces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16016/3030064003.py\u001b[0m in \u001b[0;36mget_flattened_embeddings\u001b[0;34m(outputs, attention_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m                             outputs.hidden_states[-4]), dim=2)\n\u001b[1;32m      7\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "with open (\"../data/raw/sample.jsonl\") as fin, open (\"../data/contextual-embeddings/sample.tsv\", \"w\") as fout:\n",
    "    for line in fin:\n",
    "        js = json.loads (line.strip())\n",
    "        # extract text\n",
    "        text = js[\"full_text\"] # extract additional metadata for later\n",
    "        paper_id = js[\"paper_id\"]\n",
    "        # encode the entire text\n",
    "        encoded_input = lm.tokenizer(text,\n",
    "                                     add_special_tokens=False,\n",
    "                                     return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad ():\n",
    "            # print (encoded_input[\"input_ids\"].size()) # contains approx. these many tokens\n",
    "            input_dict = split2chunks (encoded_input)\n",
    "            outputs = lm.model(**input_dict)\n",
    "            embeddings = get_flattened_embeddings (outputs, input_dict[\"attention_mask\"])\n",
    "            wordpieces = lm.tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "            tokens = [token for token in tokens_generator(wordpieces)]\n",
    "            tokenized_text = [token for _,_,token in tokens]\n",
    "            token_boundaries = [(start, ended) for start, ended, _ in tokens]\n",
    "            token_embeddings = torch.stack([embeddings[start:end,:].mean(dim=0) for start, end in token_boundaries])\n",
    "        \n",
    "        for i, token in enumerate (tokenized_text):\n",
    "            string_rep = ' '.join(list(map(str,token_embeddings[i].tolist())))\n",
    "            fout.write (f'{paper_id}\\t{i}\\t{token}\\t{string_rep}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "94091cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_flattened_embeddings (outputs, input_dict[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "395d9249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5610, 3072])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a2ddd675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5610"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpieces = lm.tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "tokens = [token for token in tokens_generator(wordpieces)]\n",
    "tokenized_text = [token for _,_,token in tokens]\n",
    "token_boundaries = [(start, ended) for start, ended, _ in tokens]\n",
    "token_embeddings = torch.stack([embeddings[start:end,:].mean(dim=0) for start, end in token_boundaries])\n",
    "len(wordpieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ac19f701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input_dict[\"attention_mask\"][:,1:-1].flatten() == 0).nonzero(as_tuple=True)[0].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "2feccb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4687\n",
      "4687\n",
      "torch.Size([4687, 3072])\n"
     ]
    }
   ],
   "source": [
    "print(len (tokenized_text))\n",
    "print(len (token_boundaries))\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e5a06a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_boundaries = [(start, ended) for start, ended, token in tokens_generator (toks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3f2630be",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "remapped_toks = list ()\n",
    "token_start = 0\n",
    "token_ended = 0\n",
    "while i < len (toks):\n",
    "    if toks[i].startswith (\"##\"):\n",
    "        remapped_toks[-1] = remapped_toks[-1] + toks[i][2:]\n",
    "        token_ended += 1\n",
    "    else:\n",
    "        remapped_toks.append (toks[i])\n",
    "        # reset start of token\n",
    "        token_start = i\n",
    "        token_ended = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d1fd3b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['previous', 'work', 'has', 'shown', 'that', 'the', 'problem', 'of', 'structural', 'differences', 'between', 'language', 'pairs', 'in', 'sm', '##t', 'can', 'be', 'alleviate', '##d', 'by', 'source', '-', 'side', 'syn', '##ta', '##ctic', 're', '##ord', '##ering', '.', 'taking', 'account', 'for', 'the', 'integration', 'with', 'sm', '##t', 'systems', ',', 'these', 'methods', 'can', 'be', 'divided', 'into', 'two', 'different', 'kinds', 'of', 'approaches', ':', 'the', 'deter', '##mini', '##stic', 're', '##ord', '##ering', 'and', 'the', 'non', '##de', '##ter', '##mini', '##stic', 're', '##ord', '##ering', 'approach', '.', 'to', 'carry', 'out', 'the', 'deter', '##mini', '##stic', 'approach', ',', 'syn', '##ta', '##ctic', 're', '##ord', '##ering', 'is', 'performed', 'uniformly', 'on', 'the', 'training', ',', 'dev', '##set', 'and', 'tests', '##et', 'before']\n",
      "['previous', 'work', 'has', 'shown', 'that', 'the', 'problem', 'of', 'structural', 'differences', 'between', 'language', 'pairs', 'in', 'smt', 'can', 'be', 'alleviated', 'by', 'source', '-', 'side', 'syntactic', 'reordering', '.', 'taking', 'account', 'for', 'the', 'integration', 'with', 'smt', 'systems', ',', 'these', 'methods', 'can', 'be', 'divided', 'into', 'two', 'different', 'kinds', 'of', 'approaches', ':', 'the', 'deterministic', 'reordering', 'and', 'the', 'nondeterministic', 'reordering', 'approach', '.', 'to', 'carry', 'out', 'the', 'deterministic', 'approach', ',', 'syntactic', 'reordering', 'is', 'performed', 'uniformly', 'on', 'the', 'training', ',', 'devset', 'and', 'testset', 'before', 'being', 'fed', 'into', 'the', 'smt', 'systems', ',', 'so', 'that', 'only', 'the', 'reordered', 'source', 'sentences', 'are', 'dealt', 'with', 'while', 'building', 'during', 'the', 'smt', 'system', '.', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(toks[0:100])\n",
    "print(remapped_toks[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bab3b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##t\n",
      "##d\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##t\n",
      "##mini\n",
      "##stic\n",
      "##ord\n",
      "##ering\n",
      "##de\n",
      "##ter\n",
      "##mini\n",
      "##stic\n",
      "##ord\n",
      "##ering\n",
      "##mini\n",
      "##stic\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##set\n",
      "##et\n",
      "##t\n",
      "##ord\n",
      "##ered\n",
      "##t\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##se\n",
      "##ord\n",
      "##ered\n",
      "##t\n",
      "##ders\n",
      "##ona\n",
      "##izan\n",
      "##ifiers\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##mini\n",
      "##stic\n",
      "##ders\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ered\n",
      "##s\n",
      "##ta\n",
      "##ctic\n",
      "##t\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ser\n",
      "##der\n",
      "##ta\n",
      "##ctic\n",
      "##ara\n",
      "##bic\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##b\n",
      "##t\n",
      "##eng\n",
      "##lish\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##mt\n",
      "##bs\n",
      "##mt\n",
      "##xa\n",
      "##mined\n",
      "##ord\n",
      "##ering\n",
      "##de\n",
      "##ter\n",
      "##mini\n",
      "##stic\n",
      "##st\n",
      "##ru\n",
      "##ction\n",
      "##osition\n",
      "##bank\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##t\n",
      "##mini\n",
      "##stic\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##une\n",
      "##d\n",
      "##ate\n",
      "##mt\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##tonic\n",
      "##s\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##se\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##b\n",
      "##t\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ness\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##a\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##tonic\n",
      "##s\n",
      "##se\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##tonic\n",
      "##s\n",
      "##lets\n",
      "##lets\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##t\n",
      "##s\n",
      "##ding\n",
      "##le\n",
      "##u\n",
      "##mt\n",
      "##s\n",
      "##s\n",
      "##ide\n",
      "##s\n",
      "##ding\n",
      "##le\n",
      "##u\n",
      "##s\n",
      "##s\n",
      "##s\n",
      "##s\n",
      "##tonic\n",
      "##ord\n",
      "##ering\n",
      "##tonic\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##lets\n",
      "##se\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##lets\n",
      "##ord\n",
      "##ering\n",
      "##ping\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##tonic\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##tonic\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##se\n",
      "##se\n",
      "##ser\n",
      "##ari\n",
      "##zation\n",
      "##se\n",
      "##ari\n",
      "##zation\n",
      "##ord\n",
      "##ering\n",
      "##se\n",
      "##se\n",
      "##let\n",
      "##is\n",
      "##fies\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##er\n",
      "##tree\n",
      "##s\n",
      "##lets\n",
      "##se\n",
      "##st\n",
      "##ru\n",
      "##cture\n",
      "##s\n",
      "##lets\n",
      "##s\n",
      "##se\n",
      "##let\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ari\n",
      "##zation\n",
      "##let\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##o\n",
      "##ord\n",
      "##ering\n",
      "##let\n",
      "##se\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##o\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ta\n",
      "##ctic\n",
      "##a\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##eng\n",
      "##lish\n",
      "##ing\n",
      "##ta\n",
      "##ctic\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##s\n",
      "##ord\n",
      "##ers\n",
      "##osition\n",
      "##nst\n",
      "##ru\n",
      "##ctions\n",
      "##a\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##mt\n",
      "##set\n",
      "##et\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##let\n",
      "##o\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##o\n",
      "##−1\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##ba\n",
      "##bilities\n",
      "##s\n",
      "##ling\n",
      "##ual\n",
      "##c\n",
      "##c\n",
      "##200\n",
      "##3\n",
      "##e\n",
      "##14\n",
      "##t\n",
      "##oll\n",
      "##ion\n",
      "##er\n",
      "##set\n",
      "##et\n",
      "##s\n",
      "##za\n",
      "##t\n",
      "##lm\n",
      "##s\n",
      "##st\n",
      "##st\n",
      "##st\n",
      "##et\n",
      "##2\n",
      "##set\n",
      "##st\n",
      "##et\n",
      "##et\n",
      "##s\n",
      "##set\n",
      "##et\n",
      "##st\n",
      "##s\n",
      "##b\n",
      "##t\n",
      "##s\n",
      "##set\n",
      "##s\n",
      "##s\n",
      "##se\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##1\n",
      "##ord\n",
      "##ering\n",
      "##ter\n",
      "##mina\n",
      "##l\n",
      "##une\n",
      "##d\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##r\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##mt\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##6\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##2\n",
      "##ord\n",
      "##ering\n",
      "##une\n",
      "##d\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##mt\n",
      "##s\n",
      "##s\n",
      "##nst\n",
      "##ru\n",
      "##ction\n",
      "##osition\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##set\n",
      "##et\n",
      "##s\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##mt\n",
      "##ord\n",
      "##ering\n",
      "##cal\n",
      "##ord\n",
      "##ering\n",
      "##set\n",
      "##s\n",
      "##s\n",
      "##st\n",
      "##s\n",
      "##mt\n",
      "##st\n",
      "##et\n",
      "##s\n",
      "##mt\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ord\n",
      "##ering\n",
      "##ate\n",
      "##s\n",
      "##s\n",
      "##et\n",
      "##e\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##per\n",
      "##forms\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##lined\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##lined\n",
      "##le\n",
      "##u\n",
      "##lined\n",
      "##st\n",
      "##lined\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##per\n",
      "##forms\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##per\n",
      "##forms\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##or\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##grade\n",
      "##s\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##s\n",
      "##per\n",
      "##form\n",
      "##st\n",
      "##st\n",
      "##et\n",
      "##e\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##per\n",
      "##forms\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##or\n",
      "##per\n",
      "##form\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##lined\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##lined\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##or\n",
      "##lined\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##per\n",
      "##forms\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##per\n",
      "##forms\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##le\n",
      "##u\n",
      "##st\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##st\n",
      "##per\n",
      "##form\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##mt\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##s\n",
      "##se\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##t\n",
      "##unes\n",
      "##s\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ding\n",
      "##t\n",
      "##osition\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##osition\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##s\n",
      "##se\n",
      "##mt\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##ta\n",
      "##ctic\n",
      "##ord\n",
      "##ering\n",
      "##per\n",
      "##form\n",
      "##unes\n",
      "##fi\n",
      "##lter\n",
      "##ed\n",
      "##et\n",
      "##s\n",
      "##st\n",
      "##et\n",
      "##ed\n",
      "##ora\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tok in toks:\n",
    "    if tok.startswith (\"##\"):\n",
    "        print (tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "792bf7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2wordpieces = dict ()\n",
    "for token in tokenized_text:\n",
    "    if token in word2wordpieces:\n",
    "        wordpieces = word2wordpieces[token]\n",
    "    else:\n",
    "        wordpieces = lm.tokenizer (token, add_special_tokens=False)[\"input_ids\"]\n",
    "        word2wordpieces[token] = wordpieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "796d4365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Previous': [3025],\n",
       " 'work': [2147],\n",
       " 'has': [2038],\n",
       " 'shown': [3491],\n",
       " 'that': [2008],\n",
       " 'the': [1996],\n",
       " 'problem': [3291],\n",
       " 'of': [1997],\n",
       " 'structural': [8332],\n",
       " 'differences': [5966],\n",
       " 'between': [2090],\n",
       " 'language': [2653],\n",
       " 'pairs': [7689],\n",
       " 'in': [1999],\n",
       " 'SMT': [15488, 2102],\n",
       " 'can': [2064],\n",
       " 'be': [2022],\n",
       " 'alleviated': [24251, 2094],\n",
       " 'by': [2011],\n",
       " 'source-side': [3120, 1011, 2217],\n",
       " 'syntactic': [19962, 2696, 13306],\n",
       " 'reordering.': [2128, 8551, 7999, 1012],\n",
       " 'Taking': [2635],\n",
       " 'account': [4070],\n",
       " 'for': [2005],\n",
       " 'integration': [8346],\n",
       " 'with': [2007],\n",
       " 'systems,': [3001, 1010],\n",
       " 'these': [2122],\n",
       " 'methods': [4725],\n",
       " 'divided': [4055],\n",
       " 'into': [2046],\n",
       " 'two': [2048],\n",
       " 'different': [2367],\n",
       " 'kinds': [7957],\n",
       " 'approaches': [8107],\n",
       " ':': [1024],\n",
       " 'deterministic': [28283, 25300, 10074],\n",
       " 'reordering': [2128, 8551, 7999],\n",
       " 'and': [1998],\n",
       " 'nondeterministic': [2512, 3207, 3334, 25300, 10074],\n",
       " 'approach.': [3921, 1012],\n",
       " 'To': [2000],\n",
       " 'carry': [4287],\n",
       " 'out': [2041],\n",
       " 'approach,': [3921, 1010],\n",
       " 'is': [2003],\n",
       " 'performed': [2864],\n",
       " 'uniformly': [27423],\n",
       " 'on': [2006],\n",
       " 'training,': [2731, 1010],\n",
       " 'devset': [16475, 13462],\n",
       " 'testset': [5852, 3388],\n",
       " 'before': [2077],\n",
       " 'being': [2108],\n",
       " 'fed': [7349],\n",
       " 'so': [2061],\n",
       " 'only': [2069],\n",
       " 'reordered': [2128, 8551, 6850],\n",
       " 'source': [3120],\n",
       " 'sentences': [11746],\n",
       " 'are': [2024],\n",
       " 'dealt': [9411],\n",
       " 'while': [2096],\n",
       " 'building': [2311],\n",
       " 'during': [2076],\n",
       " 'system.': [2291, 1012],\n",
       " 'In': [1999],\n",
       " 'this': [2023],\n",
       " 'case,': [2553, 1010],\n",
       " 'most': [2087],\n",
       " 'focused': [4208],\n",
       " 'to': [2000],\n",
       " 'extract': [14817],\n",
       " 'apply': [6611],\n",
       " 'patterns': [7060],\n",
       " 'which': [2029],\n",
       " 'come': [2272],\n",
       " 'from': [2013],\n",
       " 'manually': [21118],\n",
       " 'created': [2580],\n",
       " 'rules': [3513],\n",
       " ',': [1010],\n",
       " 'or': [2030],\n",
       " 'via': [3081],\n",
       " 'an': [2019],\n",
       " 'automatic': [6882],\n",
       " 'extraction': [14676],\n",
       " 'process': [2832],\n",
       " 'taking': [2635],\n",
       " 'advantage': [5056],\n",
       " 'parse': [11968, 3366],\n",
       " 'trees': [3628],\n",
       " '.': [1012],\n",
       " 'Because': [2138],\n",
       " 'sentence': [6251],\n",
       " 'cannot': [3685],\n",
       " 'undone': [25757],\n",
       " 'decoders': [21933, 13375],\n",
       " '(AlOnaizan': [1006, 2632, 7856, 27334],\n",
       " 'et': [3802],\n",
       " 'al.,': [2632, 1012, 1010],\n",
       " '2006)': [2294, 1007],\n",
       " 'implies': [12748],\n",
       " 'a': [1037],\n",
       " 'systematic': [11778],\n",
       " 'error': [7561],\n",
       " 'classifiers': [2465, 28295],\n",
       " 'utilized': [12550],\n",
       " 'obtain': [6855],\n",
       " 'high-performance': [2152, 1011, 2836],\n",
       " 'some': [2070],\n",
       " 'specialized': [7772],\n",
       " 'structures': [5090],\n",
       " '(e.g.': [1006, 1041, 1012, 1043, 1012],\n",
       " 'DE': [2139],\n",
       " 'construction': [2810],\n",
       " 'Chinese).': [2822, 1007, 1012],\n",
       " 'On': [2006],\n",
       " 'other': [2060],\n",
       " 'hand,': [2192, 1010],\n",
       " 'non-deterministic': [2512, 1011, 28283, 25300, 10074],\n",
       " 'approach': [3921],\n",
       " 'leaves': [3727],\n",
       " 'decisions': [6567],\n",
       " 'choose': [5454],\n",
       " 'appropriate': [6413],\n",
       " 'reorderings.': [2128, 8551, 7999, 2015, 1012],\n",
       " 'This': [2023],\n",
       " 'more': [2062],\n",
       " 'flexible': [12379],\n",
       " 'because': [2138],\n",
       " 'both': [2119],\n",
       " 'original': [2434],\n",
       " 'presented': [3591],\n",
       " 'inputs.': [20407, 1012],\n",
       " 'Word': [2773],\n",
       " 'lattices': [17779, 2015],\n",
       " 'generated': [7013],\n",
       " 'N-gram-based': [1050, 1011, 13250, 1011, 2241],\n",
       " 'chunks': [24839],\n",
       " 'POS': [13433, 2015],\n",
       " 'tags': [22073],\n",
       " 'used': [2109],\n",
       " 'rules,': [3513, 1010],\n",
       " 'word': [2773],\n",
       " 'weighted': [18215],\n",
       " 'models': [4275],\n",
       " 'models.': [4275, 1012],\n",
       " 'Rules': [3513],\n",
       " 'parser': [11968, 8043],\n",
       " 'also': [2036],\n",
       " 'form': [2433],\n",
       " 'n-best': [1050, 1011, 2190],\n",
       " 'lists': [7201],\n",
       " 'decoder': [21933, 4063],\n",
       " 'Furthermore,': [7297, 1010],\n",
       " 'Elm-ing,': [17709, 1011, 13749, 1010],\n",
       " '2009': [2268],\n",
       " ')': [1007],\n",
       " 'uses': [3594],\n",
       " 'score': [3556],\n",
       " 'output': [6434],\n",
       " 'order,': [2344, 1010],\n",
       " 'English-Danish': [2394, 1011, 5695],\n",
       " 'EnglishArabic': [2394, 5400, 13592],\n",
       " 'tasks.': [8518, 1012],\n",
       " 'Syntactic': [19962, 2696, 13306],\n",
       " 'information': [2592],\n",
       " 'considered': [2641],\n",
       " 'as': [2004],\n",
       " 'extra': [4469],\n",
       " 'feature': [3444],\n",
       " 'improve': [5335],\n",
       " 'PB-SMT': [1052, 2497, 1011, 15488, 2102],\n",
       " 'ChineseEnglish': [2822, 13159, 13602],\n",
       " 'task.': [4708, 1012],\n",
       " 'These': [2122],\n",
       " 'results': [3463],\n",
       " 'confirmed': [4484],\n",
       " 'effectiveness': [12353],\n",
       " 'However,': [2174, 1010],\n",
       " 'particular': [3327],\n",
       " 'case': [2553],\n",
       " 'Chinese': [2822],\n",
       " 'inputs,': [20407, 1010],\n",
       " 'although': [2348],\n",
       " 'been': [2042],\n",
       " 'addressed': [8280],\n",
       " 'PBSMT': [13683, 20492],\n",
       " 'HPBSMT': [6522, 5910, 20492],\n",
       " 'systems': [3001],\n",
       " 'indicated': [5393],\n",
       " 'there': [2045],\n",
       " 'still': [2145],\n",
       " 'lots': [7167],\n",
       " 'unexamined': [16655, 18684, 25089],\n",
       " 'imply': [19515],\n",
       " 'reordering,': [2128, 8551, 7999, 1010],\n",
       " 'especially': [2926],\n",
       " 'As': [2004],\n",
       " 'specified': [9675],\n",
       " 'include': [2421],\n",
       " 'bei-construction,': [21388, 1011, 2810, 1010],\n",
       " 'baconstruction,': [11611, 3367, 6820, 7542, 1010],\n",
       " 'three': [2093],\n",
       " 'de-construction': [2139, 1011, 2810],\n",
       " '(including': [1006, 2164],\n",
       " 'construction)': [2810, 1007],\n",
       " 'general': [2236],\n",
       " 'preposition': [17463, 19234],\n",
       " 'constructions.': [21913, 1012],\n",
       " 'Such': [2107],\n",
       " 'referred': [3615],\n",
       " 'functional': [8360],\n",
       " 'words': [2616],\n",
       " 'paper,': [3259, 1010],\n",
       " 'all': [2035],\n",
       " 'constructions': [21913],\n",
       " 'identified': [4453],\n",
       " 'their': [2037],\n",
       " 'corresponding': [7978],\n",
       " 'Penn': [9502],\n",
       " 'TreeBank.': [3392, 9299, 1012],\n",
       " 'It': [2009],\n",
       " 'interesting': [5875],\n",
       " 'investigate': [8556],\n",
       " 'task': [4708],\n",
       " 'since': [2144],\n",
       " 'them': [2068],\n",
       " 'tend': [7166],\n",
       " 'produce': [3965],\n",
       " 'target': [4539],\n",
       " 'sentences.': [11746, 1012],\n",
       " 'Another': [2178],\n",
       " 'related': [3141],\n",
       " 'filter': [11307],\n",
       " 'bilingual': [17636],\n",
       " 'phrase': [7655],\n",
       " 'closed-class': [2701, 1011, 2465],\n",
       " ').': [1007, 1012],\n",
       " 'By': [2011],\n",
       " 'alignments': [12139, 2015],\n",
       " 'types,': [4127, 1010],\n",
       " 'filtering': [22910],\n",
       " 'reduces': [13416],\n",
       " 'tables': [7251],\n",
       " 'up': [2039],\n",
       " 'third,': [2353, 1010],\n",
       " 'but': [2021],\n",
       " 'provide': [3073],\n",
       " 'system': [2291],\n",
       " 'competitive': [6975],\n",
       " 'performance': [2836],\n",
       " 'compared': [4102],\n",
       " 'baseline.': [26163, 1012],\n",
       " 'Similarly,': [6660, 1010],\n",
       " 'our': [2256],\n",
       " 'idea': [2801],\n",
       " 'use': [2224],\n",
       " 'special': [2569],\n",
       " 'type': [2828],\n",
       " 'purpose': [3800],\n",
       " 'patterns.': [7060, 1012],\n",
       " 'objective': [7863],\n",
       " 'exploit': [18077],\n",
       " 'Chinese-English': [2822, 1011, 2394],\n",
       " 'Our': [2256],\n",
       " 'assumption': [11213],\n",
       " 'effective': [4621],\n",
       " 'ones,': [3924, 1010],\n",
       " 'others': [2500],\n",
       " 'pruned': [10975, 9816, 2094],\n",
       " 'speed': [3177],\n",
       " 'performance.': [2836, 1012],\n",
       " 'validate': [9398, 3686],\n",
       " 'assumption,': [11213, 1010],\n",
       " 'paper:': [3259, 1024],\n",
       " 'baseline': [26163],\n",
       " 'system,': [2291, 1010],\n",
       " 'extracted': [15901],\n",
       " 'corpus,': [13931, 1010],\n",
       " 'filtered': [21839],\n",
       " 'words.': [2616, 1012],\n",
       " 'accomplish': [14570],\n",
       " 'this,': [2023, 1010],\n",
       " 'firstly': [15847],\n",
       " 'lattice': [17779],\n",
       " 'scoring': [4577],\n",
       " 'discover': [7523],\n",
       " 'non-monotonic': [2512, 1011, 18847, 25009],\n",
       " 'alignments,': [12139, 2015, 1010],\n",
       " 'then': [2059],\n",
       " 'trees.': [3628, 1012],\n",
       " 'After': [2044],\n",
       " 'that,': [2008, 1010],\n",
       " 'adopted': [4233],\n",
       " 'perform': [4685],\n",
       " 'pattern': [5418],\n",
       " 'filtering.': [22910, 1012],\n",
       " 'Finally,': [2633, 1010],\n",
       " 'unfiltered': [4895, 8873, 21928, 2098],\n",
       " 'set': [2275],\n",
       " 'one': [2028],\n",
       " 'transform': [10938],\n",
       " 'inputs': [20407],\n",
       " 'present': [2556],\n",
       " 'potential': [4022],\n",
       " 'reorderings': [2128, 8551, 7999, 2015],\n",
       " 'improving': [9229],\n",
       " 'A': [1037],\n",
       " 'comparison': [7831],\n",
       " 'carried': [3344],\n",
       " 'examine': [11628],\n",
       " 'well': [2092],\n",
       " 'usefulness': [6179, 2791],\n",
       " 'The': [1996],\n",
       " 'rest': [2717],\n",
       " 'paper': [3259],\n",
       " 'organized': [4114],\n",
       " 'follows:': [4076, 1024],\n",
       " 'section': [2930],\n",
       " '2': [1016],\n",
       " 'we': [2057],\n",
       " 'describe': [6235],\n",
       " 'patterns,': [7060, 1010],\n",
       " 'including': [2164],\n",
       " 'procedures.': [8853, 1012],\n",
       " 'Then': [2059],\n",
       " '3': [1017],\n",
       " 'presents': [7534],\n",
       " '4': [1018],\n",
       " 'shows': [3065],\n",
       " 'generation': [4245],\n",
       " 'experimental': [6388],\n",
       " 'setup': [16437],\n",
       " 'included': [2443],\n",
       " 'discussion': [6594],\n",
       " '5.': [1019, 1012],\n",
       " 'give': [2507],\n",
       " 'conclusion': [7091],\n",
       " 'avenues': [20859],\n",
       " 'future': [2925],\n",
       " '6.': [1020, 1012],\n",
       " 'Instead': [2612],\n",
       " 'top-down': [2327, 1011, 2091],\n",
       " 'such': [2107],\n",
       " 'Chang': [11132],\n",
       " '2009a)': [2268, 2050, 1007],\n",
       " 'bottom-up': [3953, 1011, 2039],\n",
       " 'similar': [2714],\n",
       " 'following': [2206],\n",
       " 'steps': [4084],\n",
       " 'patterns:': [7060, 1024],\n",
       " '1)': [1015, 1007],\n",
       " 'proposed': [3818],\n",
       " 'training': [2731],\n",
       " 'corpus;': [13931, 1025],\n",
       " '2)': [1016, 1007],\n",
       " 'regions': [4655],\n",
       " 'identify': [6709],\n",
       " 'minimum': [6263],\n",
       " 'treelets': [3392, 13461],\n",
       " 'extraction;': [14676, 1025],\n",
       " '3)': [1017, 1007],\n",
       " 'transformed': [8590],\n",
       " 'occurrences': [27247],\n",
       " 'corpus.': [13931, 1012],\n",
       " 'Details': [4751],\n",
       " 'each': [2169],\n",
       " 'section.': [2930, 1012],\n",
       " 'data': [2951],\n",
       " 'cleaning': [9344],\n",
       " 'clean': [4550],\n",
       " 'approximate': [15796],\n",
       " 'decoding': [21933, 4667],\n",
       " 'results,': [3463, 1010],\n",
       " 'calculate': [18422],\n",
       " 'BLEU': [1038, 2571, 2226],\n",
       " 'scores': [7644],\n",
       " 'low-scoring': [2659, 1011, 4577],\n",
       " 'pairs.': [7689, 1012],\n",
       " 'taken': [2579],\n",
       " 'approach:': [3921, 1024],\n",
       " 'train': [3345],\n",
       " 'initial': [3988],\n",
       " 'model;': [2944, 1025],\n",
       " 'collect': [8145],\n",
       " 'anchor': [8133],\n",
       " 'containing': [4820],\n",
       " 'positions': [4460],\n",
       " 'phase;': [4403, 1025],\n",
       " 'build': [3857],\n",
       " 'translation': [5449],\n",
       " '4)': [1018, 1007],\n",
       " 'search': [3945],\n",
       " 'sourceside': [4216, 5178],\n",
       " 'results;': [3463, 1025],\n",
       " '5)': [1019, 1007],\n",
       " 'cleaning.': [9344, 1012],\n",
       " 'Note': [3602],\n",
       " 'step': [3357],\n",
       " 'pairs,': [7689, 1010],\n",
       " 'edge': [3341],\n",
       " 'contain': [5383],\n",
       " 'positions.': [4460, 1012],\n",
       " 'Thus': [2947],\n",
       " 'outputs': [27852],\n",
       " 'areas': [2752],\n",
       " 'Non-monotonic': [2512, 1011, 18847, 25009],\n",
       " 'examined': [8920],\n",
       " 'mapped': [17715],\n",
       " 'treelets.': [3392, 13461, 1012],\n",
       " 'B': [1038],\n",
       " 'indicating': [8131],\n",
       " 'swapping': [19948, 4691],\n",
       " 'operations': [3136],\n",
       " 'side': [2217],\n",
       " 'Thus,': [2947, 1010],\n",
       " 'given': [2445],\n",
       " 'AB,': [11113, 1010],\n",
       " '(1):': [1006, 1015, 1007, 1024],\n",
       " 'sequences.': [10071, 1012],\n",
       " 'Referring': [7727],\n",
       " 'alignment': [12139],\n",
       " 'last': [2197],\n",
       " 'section,': [2930, 1010],\n",
       " 'produces': [7137],\n",
       " 'region.': [2555, 1012],\n",
       " 'region': [2555],\n",
       " 'identified,': [4453, 1010],\n",
       " 'its': [2049],\n",
       " 'sub-areas': [4942, 1011, 2752],\n",
       " 'attempted': [4692],\n",
       " 'regions.': [4655, 1012],\n",
       " 'represent': [5050],\n",
       " 'using': [2478],\n",
       " 'structure,': [3252, 1010],\n",
       " 'map': [4949],\n",
       " 'onto': [3031],\n",
       " 'trees,': [3628, 1010],\n",
       " 'generate': [9699],\n",
       " '1.': [1015, 1012],\n",
       " 'Generate': [9699],\n",
       " 'tree': [3392],\n",
       " 'Berkeley': [8256],\n",
       " 'paper.': [3259, 1012],\n",
       " 'simpler': [16325],\n",
       " 'structures,': [5090, 1010],\n",
       " 'right-binarization': [2157, 1011, 8026, 8486, 9276],\n",
       " 'binarization': [8026, 8486, 9276],\n",
       " 'not': [2025],\n",
       " 'distinguished': [5182],\n",
       " 'ones': [3924],\n",
       " '@V': [1030, 1058],\n",
       " 'P': [1052],\n",
       " 'V': [1058],\n",
       " 'same).': [2168, 1007, 1012],\n",
       " '2.': [1016, 1012],\n",
       " 'Map': [4949],\n",
       " 'AB': [11113],\n",
       " 'Denote': [19090],\n",
       " 'N': [1050],\n",
       " 'leaf': [7053],\n",
       " 'nodes': [14164],\n",
       " 'B.': [1038, 1012],\n",
       " 'mapping': [12375],\n",
       " 'find': [2424],\n",
       " 'treelet': [3392, 7485],\n",
       " 'T': [1056],\n",
       " 'satisfies': [2938, 2483, 14213],\n",
       " 'criteria:': [9181, 1024],\n",
       " 'must': [2442],\n",
       " 'exist': [4839],\n",
       " 'path': [4130],\n",
       " 'node': [13045],\n",
       " '∪': [1605],\n",
       " 'root': [7117],\n",
       " ';': [1025],\n",
       " 'ancestor': [13032],\n",
       " '(or': [1006, 2030],\n",
       " 'none': [3904],\n",
       " 'them).': [2068, 1007, 1012],\n",
       " '3.': [1017, 1012],\n",
       " 'Traverse': [20811],\n",
       " 'pre-order': [3653, 1011, 2344],\n",
       " 'Label': [3830],\n",
       " 'reorder': [2128, 8551, 2121],\n",
       " 'options,': [7047, 1010],\n",
       " 'indicate': [5769],\n",
       " 'descendants': [8481],\n",
       " 'label': [3830],\n",
       " 'supposed': [4011],\n",
       " 'swapped': [29176],\n",
       " 'those': [2216],\n",
       " 'subtrees,': [4942, 13334, 2015, 1010],\n",
       " 'refer': [6523],\n",
       " 'located': [2284],\n",
       " 'substructures,': [4942, 3367, 6820, 14890, 2015, 1010],\n",
       " 'do': [2079],\n",
       " 'necessarily': [9352],\n",
       " 'go': [2175],\n",
       " 'down': [2091],\n",
       " 'nodes.': [14164, 1012],\n",
       " 'Since': [2144],\n",
       " 'always': [2467],\n",
       " 'perfectly': [6669],\n",
       " 'matched': [10349],\n",
       " 'expand': [7818],\n",
       " 'right': [2157],\n",
       " 'and/or': [1998, 1013, 2030],\n",
       " 'left': [2187],\n",
       " 'limited': [3132],\n",
       " 'number': [2193],\n",
       " 'treelet.': [3392, 7485, 1012],\n",
       " 'situation,': [3663, 1010],\n",
       " 'ancestors': [10748],\n",
       " 'expanded': [4423],\n",
       " 'kept': [2921],\n",
       " 'they': [2027],\n",
       " 'assigned': [4137],\n",
       " 'same': [2168],\n",
       " 'labels': [10873],\n",
       " 'have': [2031],\n",
       " 'expanded.': [4423, 1012],\n",
       " 'context': [6123],\n",
       " 'Figure': [3275],\n",
       " 'illustrates': [24899],\n",
       " 'process.': [2832, 1012],\n",
       " 'symbol': [6454],\n",
       " '@': [1030],\n",
       " 'indicates': [7127],\n",
       " 'symbols': [9255],\n",
       " 'figure).': [3275, 1007, 1012],\n",
       " 'figure,': [3275, 1010],\n",
       " '(surrounded': [1006, 5129],\n",
       " 'dashed': [18198],\n",
       " 'lines)': [3210, 1007],\n",
       " 'AB.': [11113, 1012],\n",
       " 'Leaf': [7053],\n",
       " 'labeled': [12599],\n",
       " 'A,': [1037, 1010],\n",
       " 'B,': [1038, 1010],\n",
       " 'A.': [1037, 1012],\n",
       " 'collected': [5067],\n",
       " 'sequences': [10071],\n",
       " 'L': [1048],\n",
       " 'op-': [6728, 1011],\n",
       " '(2):': [1006, 1016, 1007, 1024],\n",
       " '(2)': [1006, 1016, 1007],\n",
       " 'where': [2073],\n",
       " 'first': [2034],\n",
       " 'part': [2112],\n",
       " 'second': [2117],\n",
       " 'O': [1051],\n",
       " 'scheme,': [5679, 1010],\n",
       " 'We': [2057],\n",
       " 'p': [1052],\n",
       " 'reo': [2128, 2080],\n",
       " 'chance': [3382],\n",
       " 'when': [2043],\n",
       " 'tree.': [3392, 1012],\n",
       " 'estimated': [4358],\n",
       " '(3):': [1006, 1017, 1007, 1024],\n",
       " 'contrast,': [5688, 1010],\n",
       " 'usually': [2788],\n",
       " 'contains': [3397],\n",
       " 'several': [2195],\n",
       " 'schemes': [11683],\n",
       " '(specified': [1006, 9675],\n",
       " 'formula': [5675],\n",
       " '(2)),': [1006, 1016, 1007, 1007, 1010],\n",
       " '(4):': [1006, 1018, 1007, 1024],\n",
       " 'Generally,': [3227, 1010],\n",
       " 'expressed': [5228],\n",
       " '(5):': [1006, 1019, 1007, 1024],\n",
       " 'pattern,': [5418, 1010],\n",
       " 'probability,': [9723, 1010],\n",
       " 'i': [1045],\n",
       " 'w': [1059],\n",
       " 'weights': [15871],\n",
       " '(1': [1006, 1015],\n",
       " '≤': [1608],\n",
       " 'n).': [1050, 1007, 1012],\n",
       " 'Some': [2070],\n",
       " 'may': [2089],\n",
       " 'benefit': [5770],\n",
       " 'final': [2345],\n",
       " 'controlled': [4758],\n",
       " 'rather': [2738],\n",
       " 'than': [2084],\n",
       " 'knowledge.': [3716, 1012],\n",
       " 'Inspired': [4427],\n",
       " 'study': [2817],\n",
       " '(Chang': [1006, 11132],\n",
       " '2009a;': [2268, 2050, 1025],\n",
       " 'assume': [7868],\n",
       " 'incorporate': [13265],\n",
       " 'knowledge': [3716],\n",
       " 'instead': [2612],\n",
       " 'directly': [3495],\n",
       " 'specifying': [20648, 2075],\n",
       " 'structure': [3252],\n",
       " 'linguistic': [12158],\n",
       " 'aspects,': [5919, 1010],\n",
       " 'meaningful': [15902],\n",
       " 'noise': [5005],\n",
       " 'produced': [2550],\n",
       " 'size': [2946],\n",
       " 'reduced,': [4359, 1010],\n",
       " 'improved.': [5301, 1012],\n",
       " 'Table': [2795],\n",
       " 'normally': [5373],\n",
       " 'reorders': [2128, 8551, 2545],\n",
       " 'English': [2394],\n",
       " 'de': [2139],\n",
       " '(3': [1006, 1017],\n",
       " 'rd': [16428],\n",
       " 'kind)': [2785, 1007],\n",
       " 'VP': [21210],\n",
       " 'LB': [6053],\n",
       " 'bei': [21388],\n",
       " 'long': [2146],\n",
       " 'bei-construction': [21388, 1011, 2810],\n",
       " 'excluding': [13343],\n",
       " 'ba': [8670],\n",
       " 'SB': [24829],\n",
       " 'short': [2460],\n",
       " 'deconstructions,': [21933, 23808, 6820, 22014, 1010],\n",
       " 'kind': [2785],\n",
       " 'process,': [2832, 1010],\n",
       " 'purpose.': [3800, 1012],\n",
       " 'Both': [2119],\n",
       " 'process:': [2832, 1024],\n",
       " '′': [1531],\n",
       " '{a': [1063, 1037],\n",
       " '1': [1015],\n",
       " '·': [1087],\n",
       " 'm': [1049],\n",
       " '}': [1065],\n",
       " '∈': [1596],\n",
       " '(spanning': [1006, 13912],\n",
       " '{w': [1063, 1059],\n",
       " '})': [1065, 1007],\n",
       " '{b': [1063, 1038],\n",
       " 'b': [1038],\n",
       " 'n': [1050],\n",
       " '{v': [1063, 1058],\n",
       " 'v': [1058],\n",
       " 'q': [1053],\n",
       " 'paths': [10425],\n",
       " 'lattice.': [17779, 1012],\n",
       " 'sort': [4066],\n",
       " '(5),': [1006, 1019, 1007, 1010],\n",
       " 'pre-defined': [3653, 1011, 4225],\n",
       " 'sentence.': [6251, 1012],\n",
       " 'For': [2005],\n",
       " 'node,': [13045, 1010],\n",
       " 'if': [2065],\n",
       " 'denote': [19090],\n",
       " 'E': [1041],\n",
       " '0': [1014],\n",
       " 'sentence,': [6251, 1010],\n",
       " '{P': [1063, 1052],\n",
       " 'k': [1047],\n",
       " 'applied': [4162],\n",
       " '(6):': [1006, 1020, 1007, 1024],\n",
       " '(6)': [1006, 1020, 1007],\n",
       " '(P': [1006, 1052],\n",
       " 'weight': [3635],\n",
       " '(3),': [1006, 1017, 1007, 1010],\n",
       " 'α': [1155],\n",
       " 'base': [2918],\n",
       " 'probability': [9723],\n",
       " 'avoid': [4468],\n",
       " 'equal': [5020],\n",
       " 'zero.': [5717, 1012],\n",
       " 'Suppose': [6814],\n",
       " '{E': [1063, 1041],\n",
       " 's': [1055],\n",
       " 's+r−1': [1055, 1009, 1054, 27944],\n",
       " 'r': [1054],\n",
       " 'j': [1046],\n",
       " '(7):': [1006, 1021, 1007, 1024],\n",
       " 't': [1056],\n",
       " 'scheme': [5679],\n",
       " '<=': [1026, 1027],\n",
       " '<': [1026],\n",
       " '+': [1009],\n",
       " 'r.': [1054, 1012],\n",
       " 'Reordering': [2128, 8551, 7999],\n",
       " 'share': [3745],\n",
       " 'probabilities': [4013, 3676, 14680],\n",
       " '(7).': [1006, 1021, 1007, 1012],\n",
       " 'conducted': [4146],\n",
       " 'experiments': [7885],\n",
       " 'medium-sized': [5396, 1011, 7451],\n",
       " 'corpus': [13931],\n",
       " 'FBIS': [8495, 2015],\n",
       " '(a': [1006, 1037],\n",
       " 'multilingual': [4800, 2989, 8787],\n",
       " 'paragraph-aligned': [20423, 1011, 13115],\n",
       " 'LDC': [25510, 2278],\n",
       " 'resource': [7692],\n",
       " 'LDC2003E14)': [25510, 2278, 28332, 2509, 2063, 16932, 1007],\n",
       " 'Champollion': [24782, 14511, 3258],\n",
       " 'aligner': [25705, 2121],\n",
       " 'alignment.': [12139, 1012],\n",
       " 'total': [2561],\n",
       " '256,911': [17273, 1010, 19989],\n",
       " 'obtained,': [4663, 1010],\n",
       " '2,000': [1016, 1010, 2199],\n",
       " 'randomly': [18154],\n",
       " 'selected,': [3479, 1010],\n",
       " 'call': [2655],\n",
       " 'set.': [2275, 1012],\n",
       " 'Moses': [9952],\n",
       " 'GIZA++': [21025, 4143, 1009, 1009],\n",
       " 'Minimum': [6263],\n",
       " 'rate': [3446],\n",
       " '(MERT)': [1006, 21442, 2102, 1007],\n",
       " 'tuning.': [17372, 1012],\n",
       " '5-gram': [1019, 1011, 13250],\n",
       " 'model': [2944],\n",
       " 'built': [2328],\n",
       " 'SRILM': [5185, 13728],\n",
       " 'Experiments': [7885],\n",
       " 'reported': [2988],\n",
       " 'sets:': [4520, 1024],\n",
       " 'NIST': [9152, 3367],\n",
       " 'set,': [2275, 1010],\n",
       " '2005': [2384],\n",
       " '(1,082': [1006, 1015, 1010, 5511, 2475],\n",
       " 'sentences)': [11746, 1007],\n",
       " 'devset,': [16475, 13462, 1010],\n",
       " '2008': [2263],\n",
       " '(1,357': [1006, 1015, 1010, 26231],\n",
       " 'testset.': [5852, 3388, 1012],\n",
       " 'reference': [4431],\n",
       " 'testset,': [5852, 3388, 1010],\n",
       " 'four': [2176],\n",
       " 'references.': [7604, 1012],\n",
       " 'above': [2682],\n",
       " 'alignments.': [12139, 2015, 1012],\n",
       " 'tuned': [15757],\n",
       " 'weights.': [15871, 1012],\n",
       " '2.1,': [1016, 1012, 1015, 1010],\n",
       " 'From': [2013],\n",
       " '48,285': [4466, 1010, 21777],\n",
       " '(57,861': [1006, 5401, 1010, 6564, 2487],\n",
       " 'schemes)': [11683, 1007],\n",
       " 'average': [2779],\n",
       " '11.02': [2340, 1012, 6185],\n",
       " 'non-terminals.': [2512, 1011, 17703, 1012],\n",
       " 'computational': [15078],\n",
       " 'efficiency,': [8122, 1010],\n",
       " 'any': [2151],\n",
       " 'nonterminal': [2512, 3334, 22311, 2140],\n",
       " 'less': [2625],\n",
       " '9': [1023],\n",
       " 'pruned.': [10975, 9816, 2094, 1012],\n",
       " 'procedure': [7709],\n",
       " '18,169': [2324, 1010, 18582],\n",
       " '(22,850': [1006, 2570, 1010, 15678],\n",
       " 'aver-age': [13642, 2099, 1011, 2287],\n",
       " '7.6': [1021, 1012, 1020],\n",
       " 'without': [2302],\n",
       " 'filtering,': [22910, 1010],\n",
       " 'here': [2182],\n",
       " 'after': [2044],\n",
       " \"'unfiltered\": [1005, 4895, 8873, 21928, 2098],\n",
       " \"system'.\": [2291, 1005, 1012],\n",
       " 'Using': [2478],\n",
       " 'out,': [2041, 1010],\n",
       " '6,926': [1020, 1010, 6227, 2575],\n",
       " '(with': [1006, 2007],\n",
       " '9,572': [1023, 1010, 5401, 2475],\n",
       " 'retained.': [6025, 1012],\n",
       " 'reduced': [4359],\n",
       " '61.88%,': [6079, 1012, 6070, 1003, 1010],\n",
       " 'over': [2058],\n",
       " 'half': [2431],\n",
       " 'tags.': [22073, 1012],\n",
       " \"'filtered\": [1005, 21839],\n",
       " 'Statistics': [6747],\n",
       " 'respect': [4847],\n",
       " 'types': [4127],\n",
       " 'illustrated,': [7203, 1010],\n",
       " 'percentages': [7017, 2015],\n",
       " 'reported.': [2988, 1012],\n",
       " 'word,': [2773, 1010],\n",
       " 'sum': [7680],\n",
       " 'one.': [2028, 1012],\n",
       " 'demonstrated': [7645],\n",
       " 'deconstruction': [21933, 23808, 6820, 7542],\n",
       " 'takes': [3138],\n",
       " '60.11%': [3438, 1012, 2340, 1003],\n",
       " 'main': [2364],\n",
       " 'experiment.': [7551, 1012],\n",
       " 'closely': [4876],\n",
       " '(excluding': [1006, 13343],\n",
       " 'ba)': [8670, 1007],\n",
       " 'accounts': [6115],\n",
       " '37.41%': [4261, 1012, 4601, 1003],\n",
       " 'it': [2009],\n",
       " 'major': [2350],\n",
       " 'much': [2172],\n",
       " 'smaller': [3760],\n",
       " 'amount': [3815],\n",
       " 'percentages,': [7017, 2015, 1010],\n",
       " 'minor': [3576],\n",
       " 'impact': [4254],\n",
       " 'experiments.': [7885, 1012],\n",
       " '4,': [1018, 1010],\n",
       " 'converted': [4991],\n",
       " 'respectively.': [4414, 1012],\n",
       " 'dramatic': [6918],\n",
       " 'increase': [3623],\n",
       " 'lattices,': [17779, 2015, 1010],\n",
       " 'constraints': [14679],\n",
       " 'applied:': [4162, 1024],\n",
       " 'maximum': [4555],\n",
       " '30,': [2382, 1010],\n",
       " 'span': [8487],\n",
       " '30.': [2382, 1012],\n",
       " 'construction,': [2810, 1010],\n",
       " '(7)': [1006, 1021, 1007],\n",
       " '0.05.': [1014, 1012, 5709, 1012],\n",
       " 'built-in': [2328, 1011, 1999],\n",
       " '(distance-based': [1006, 3292, 1011, 2241],\n",
       " 'lexical': [16105, 9289],\n",
       " 'reordering)': [2128, 8551, 7999, 1007],\n",
       " 'Moses,': [9952, 1010],\n",
       " 'log-linear': [8833, 1011, 7399],\n",
       " 'devsets.': [16475, 13462, 2015, 1012],\n",
       " 'effects': [3896],\n",
       " 'sets,': [4520, 1010],\n",
       " 'illustrated': [7203],\n",
       " 'table,': [2795, 1010],\n",
       " 'clear': [3154],\n",
       " 'dramatically': [12099],\n",
       " 'input': [7953],\n",
       " 'reduction': [7312],\n",
       " '37.99%': [4261, 1012, 5585, 1003],\n",
       " 'Three': [2093],\n",
       " 'set:': [2275, 1024],\n",
       " 'enabled,': [9124, 1010],\n",
       " 'values': [5300],\n",
       " 'distortion': [20870],\n",
       " 'limit': [5787],\n",
       " '(DL)': [1006, 21469, 1007],\n",
       " 'parameter': [16381],\n",
       " 'chosen': [4217],\n",
       " 'consistency.': [18700, 1012],\n",
       " 'evaluation': [9312],\n",
       " 'Results': [3463],\n",
       " '(DL': [1006, 21469],\n",
       " '=': [1027],\n",
       " 'limit,': [5787, 1010],\n",
       " 'METE=METEOR)': [2777, 2063, 1027, 23879, 1007],\n",
       " 'parameters': [11709],\n",
       " 'terms': [3408],\n",
       " 'BLEU,': [1038, 2571, 2226, 1010],\n",
       " 'METEOR': [23879],\n",
       " '(scores': [1006, 7644],\n",
       " 'bold': [7782],\n",
       " 'face).': [2227, 1007, 1012],\n",
       " 'comparable': [12435],\n",
       " 'system:': [2291, 1024],\n",
       " 'limits,': [6537, 1010],\n",
       " 'even': [2130],\n",
       " 'outperforms': [2041, 4842, 22694],\n",
       " 'face,': [2227, 1010],\n",
       " 'e.g.': [1041, 1012, 1043, 1012],\n",
       " 'DL=12,': [21469, 1027, 2260, 1010],\n",
       " 'DL=10).': [21469, 1027, 2184, 1007, 1012],\n",
       " 'best': [2190],\n",
       " 'obtained': [4663],\n",
       " '12': [2260],\n",
       " '(underlined);': [1006, 2104, 18194, 1007, 1025],\n",
       " 'achieved': [4719],\n",
       " '10': [2184],\n",
       " 'accomplished': [8885],\n",
       " '(underlined),': [1006, 2104, 18194, 1007, 1010],\n",
       " '(underlined).': [1006, 2104, 18194, 1007, 1012],\n",
       " '0.41': [1014, 1012, 4601],\n",
       " '(1.67%': [1006, 1015, 1012, 6163, 1003],\n",
       " 'relative)': [5816, 1007],\n",
       " 'points,': [2685, 1010],\n",
       " '0.02': [1014, 1012, 6185],\n",
       " '(0.30%': [1006, 1014, 1012, 2382, 1003],\n",
       " 'points': [2685],\n",
       " '0.36': [1014, 1012, 4029],\n",
       " '(0.66%': [1006, 1014, 1012, 5764, 1003],\n",
       " 'points.': [2685, 1012],\n",
       " '0.34': [1014, 1012, 4090],\n",
       " '(1.38%': [1006, 1015, 1012, 4229, 1003],\n",
       " '0.53': [1014, 1012, 5187],\n",
       " '(0.98%': [1006, 1014, 1012, 5818, 1003],\n",
       " 'ME-TEOR': [2033, 1011, 8915, 2953],\n",
       " 'Compared': [4102],\n",
       " 'degrades': [2139, 24170, 2015],\n",
       " '0.07': [1014, 1012, 5718],\n",
       " '(0.28%': [1006, 1014, 1012, 2654, 1003],\n",
       " 'term': [2744],\n",
       " 'improves': [24840],\n",
       " '0.17': [1014, 1012, 2459],\n",
       " '(0.31%': [1006, 1014, 1012, 2861, 1003],\n",
       " 'METEOR,': [23879, 1010],\n",
       " 'score.': [3556, 1012],\n",
       " 'outperform': [2041, 4842, 14192],\n",
       " '5': [1019],\n",
       " 'limits': [6537],\n",
       " '6': [1020],\n",
       " 'NIST,': [9152, 3367, 1010],\n",
       " '6,': [1020, 1010],\n",
       " '1.36': [1015, 1012, 4029],\n",
       " '(8.56%': [1006, 1022, 1012, 5179, 1003],\n",
       " '0.51': [1014, 1012, 4868],\n",
       " '(8.28%': [1006, 1022, 1012, 2654, 1003],\n",
       " '1.90': [1015, 1012, 3938],\n",
       " '(4.14%': [1006, 1018, 1012, 2403, 1003],\n",
       " '1.66': [1015, 1012, 5764],\n",
       " '(10.45%': [1006, 2184, 1012, 3429, 1003],\n",
       " '0.56': [1014, 1012, 5179],\n",
       " '(9.52%': [1006, 1023, 1012, 4720, 1003],\n",
       " '2.27': [1016, 1012, 2676],\n",
       " '(4.95%': [1006, 1018, 1012, 5345, 1003],\n",
       " 'boost': [12992],\n",
       " '0.30': [1014, 1012, 2382],\n",
       " '(1.74%': [1006, 1015, 1012, 6356, 1003],\n",
       " '0.05': [1014, 1012, 5709],\n",
       " '(0.75%': [1006, 1014, 1012, 4293, 1003],\n",
       " '0.37': [1014, 1012, 4261],\n",
       " '(0.77%': [1006, 1014, 1012, 6255, 1003],\n",
       " 'METEOR.': [23879, 1012],\n",
       " 'demonstrate': [10580],\n",
       " 'significantly': [6022],\n",
       " 'previous': [3025],\n",
       " 'sections': [5433],\n",
       " 'that:': [2008, 1024],\n",
       " 'providing': [4346],\n",
       " 'trees;': [3628, 1025],\n",
       " 'play': [2377],\n",
       " 'role': [2535],\n",
       " 'maintains': [9319],\n",
       " 'prunes': [10975, 26639],\n",
       " 'whole': [2878],\n",
       " '61.88%': [6079, 1012, 6070, 1003],\n",
       " 'sizes': [10826],\n",
       " '37.99%,': [4261, 1012, 5585, 1003, 1010],\n",
       " 'thus': [2947],\n",
       " 'tuning/decoding': [17372, 1013, 21933, 4667],\n",
       " 'sped': [16887],\n",
       " 'dramatically,': [12099, 1010],\n",
       " 'make': [2191],\n",
       " 'useful': [6179],\n",
       " 'real': [2613],\n",
       " 'world,': [2088, 1010],\n",
       " 'online': [3784],\n",
       " 'systems.': [3001, 1012],\n",
       " 'statistics': [6747],\n",
       " 'argue': [7475],\n",
       " 'sources': [4216],\n",
       " 'showed': [3662],\n",
       " 'advantages': [12637],\n",
       " 'dealing': [7149],\n",
       " 'construction.': [2810, 1012],\n",
       " 'too,': [2205, 1010],\n",
       " 'though': [2295],\n",
       " 'automatically': [8073],\n",
       " 'dominate': [16083],\n",
       " 'result': [2765],\n",
       " 'confirms': [23283],\n",
       " 'highlights': [11637],\n",
       " 'importance': [5197],\n",
       " 'aim': [6614],\n",
       " 'within': [2306],\n",
       " 'compared:': [4102, 1024],\n",
       " 'Evaluation': [9312],\n",
       " 'consistently': [10862],\n",
       " 'select': [7276],\n",
       " 'obtains': [6855, 2015],\n",
       " '1.74%': [1015, 1012, 6356, 1003],\n",
       " 'relative': [5816],\n",
       " 'improvement': [7620],\n",
       " 'work,': [2147, 1010],\n",
       " 'will': [2097],\n",
       " 'investigated': [10847],\n",
       " 'fine-grained': [2986, 1011, 8982, 2098],\n",
       " 'analysis': [4106],\n",
       " 'larger': [3469],\n",
       " 'corpora': [13058, 6525],\n",
       " 'validation': [27354],\n",
       " 'method.': [4118, 1012]}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2wordpieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "23e185d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3888\n",
      "torch.Size([1, 5372])\n"
     ]
    }
   ],
   "source": [
    "print (len (tokenized_text)) # our original sequence\n",
    "print (encoded_input[\"input_ids\"].size()) # encoded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2de4a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 768])\n",
      "tensor([[-1.6916e-01, -2.3938e-02, -6.3066e-02,  ...,  3.6686e-01,\n",
      "         -2.8920e-01,  2.4723e-02],\n",
      "        [-5.8806e-02,  4.8172e-01, -9.5538e-01,  ..., -1.0929e+00,\n",
      "          3.6555e-01,  8.4185e-01],\n",
      "        [-4.7472e-01,  1.0298e+00, -1.0489e+00,  ..., -6.4436e-01,\n",
      "         -5.2447e-02,  4.9744e-01],\n",
      "        ...,\n",
      "        [-6.0153e-04, -7.5476e-01,  1.1760e-01,  ...,  7.1302e-01,\n",
      "         -8.9428e-01, -3.3192e-02],\n",
      "        [-5.5412e-01, -2.9986e-01, -3.2172e-01,  ...,  1.0814e+00,\n",
      "         -2.6647e-01,  1.2822e-01],\n",
      "        [-1.6653e-01,  2.9930e-02,  4.8501e-02,  ...,  3.3610e-01,\n",
      "         -3.5506e-01,  5.3574e-02]])\n",
      "tensor([[-0.3802, -0.2830,  0.2516,  ...,  0.3296,  0.0750, -0.0262],\n",
      "        [ 0.1706,  0.1378, -1.0766,  ..., -0.6881,  0.8039,  0.7762],\n",
      "        [-0.4868,  0.6286, -1.7008,  ..., -0.3775, -0.0198,  0.6475],\n",
      "        ...,\n",
      "        [-0.1883, -0.8068, -0.0559,  ...,  0.9973, -0.6118,  0.2201],\n",
      "        [-0.9317, -0.1211, -0.6334,  ...,  1.1118, -0.1501,  0.1274],\n",
      "        [-0.3982, -0.1924,  0.3262,  ...,  0.2985,  0.0077,  0.0268]])\n",
      "tensor([[-0.0248, -0.0335,  0.0519,  ..., -0.0503,  0.0024, -0.0625],\n",
      "        [ 0.2222,  0.2735, -0.6390,  ..., -0.6759,  0.8327,  0.4049],\n",
      "        [-0.0880,  1.4253, -1.1619,  ..., -0.3136, -0.2310,  0.4556],\n",
      "        ...,\n",
      "        [ 0.1748, -1.1478, -0.4865,  ...,  1.3359, -0.4862,  0.0559],\n",
      "        [-0.7004,  0.1359, -0.6466,  ...,  0.6506, -0.1626, -0.1871],\n",
      "        [-0.0474,  0.0338,  0.0548,  ..., -0.0228,  0.0060, -0.0361]])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[-1][0].size()) #12th hidden layer\n",
    "print(outputs.hidden_states[-2][0]) #11th hidden layer\n",
    "print(outputs.hidden_states[-3][0]) #10th hidden layer\n",
    "print(outputs.hidden_states[-4][0]) #9th hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8b78442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 512])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "45a9df8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 512])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c90b1f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f72d47be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # do this so that the costly gradients are not calculated\n",
    "    text = \"Replace me by any text you'd like.\"\n",
    "    encoded_input = lm.tokenizer(text, \n",
    "                                 add_special_tokens=False,\n",
    "                                 return_tensors='pt')\n",
    "    print (encoded_input)\n",
    "    input_ids_chunks = encoded_input['input_ids'][0].split(510)\n",
    "    mask_chunks = encoded_input['attention_mask'][0].split(510)\n",
    "    print (mask_chunks)\n",
    "    #output = lm.model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d1aa6",
   "metadata": {},
   "source": [
    "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15b7185a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cf7f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replace', 'me', 'by', 'any', 'text', 'you', \"'\", 'd', 'like', '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.tokenizer.convert_ids_to_tokens (encoded_input[\"input_ids\"][0],\n",
    "                                    skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed5e128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3072])\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.cat((output.hidden_states[-1][0], \n",
    "                        output.hidden_states[-2][0],\n",
    "                        output.hidden_states[-3][0],\n",
    "                        output.hidden_states[-4][0]), dim=1)\n",
    "embeddings = embeddings[1:-1, :]\n",
    "print (embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64eb620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1510, -0.2819,  0.5040,  ..., -0.0091,  0.0294,  0.1366],\n",
      "        [ 1.0676, -0.1254,  0.9921,  ...,  0.6519,  0.4978,  0.1428],\n",
      "        [ 0.1723, -0.2149,  0.5841,  ..., -0.5973,  0.2768,  0.8791],\n",
      "        ...,\n",
      "        [ 0.3494,  0.1025,  0.7701,  ..., -0.9383, -0.5957, -0.0526],\n",
      "        [ 0.2089, -0.3424, -0.0376,  ..., -0.0370, -0.1770, -0.5604],\n",
      "        [-0.1014, -0.1054,  0.5419,  ...,  0.0048, -0.1940,  0.0583]])\n",
      "tensor([[ 0.1486, -0.4854,  0.4405,  ..., -0.0168, -0.1848, -0.2286],\n",
      "        [ 1.3134, -0.2420,  1.1080,  ...,  0.4739, -0.1670,  0.0852],\n",
      "        [ 0.4342, -0.3302,  0.4590,  ..., -0.6905,  0.0618,  1.1912],\n",
      "        ...,\n",
      "        [ 0.1250,  0.3564,  1.0470,  ..., -1.4132,  0.0486,  0.0545],\n",
      "        [ 0.0532, -0.1248, -0.1468,  ..., -0.4602, -0.8243, -0.7510],\n",
      "        [ 0.0078, -0.2642,  0.3153,  ...,  0.0701, -0.1611, -0.2113]])\n",
      "tensor([[-0.0364, -0.4436,  0.7083,  ...,  0.2594, -0.1732, -0.1971],\n",
      "        [ 0.9671, -0.5260,  1.2343,  ...,  0.9798, -0.1338,  0.4549],\n",
      "        [ 0.5129, -0.3483,  0.1545,  ..., -0.3001, -0.1865,  1.2539],\n",
      "        ...,\n",
      "        [ 0.2375, -0.6823,  0.9421,  ..., -0.6919, -0.1746, -0.2075],\n",
      "        [-0.1384, -0.1625, -0.2017,  ..., -0.5991, -1.0491, -0.2723],\n",
      "        [-0.1567, -0.2874,  0.6088,  ...,  0.2816, -0.2020, -0.1850]])\n",
      "tensor([[-0.0137, -0.0563,  0.0337,  ..., -0.1025, -0.0707, -0.1053],\n",
      "        [ 0.7521, -0.0574,  0.8534,  ...,  1.1390,  0.1759,  0.5860],\n",
      "        [ 0.6877,  0.0468, -0.5832,  ..., -0.3160, -0.0594,  1.4781],\n",
      "        ...,\n",
      "        [ 0.4719, -0.8679,  0.0400,  ..., -0.9865,  0.0498,  0.3318],\n",
      "        [ 0.0181,  0.0938, -0.9943,  ..., -0.4979, -0.8213, -0.1097],\n",
      "        [-0.0591,  0.0535,  0.0402,  ..., -0.0609, -0.0620, -0.0486]])\n"
     ]
    }
   ],
   "source": [
    "print(output.hidden_states[-1][0]) #12th hidden layer\n",
    "print(output.hidden_states[-2][0]) #11th hidden layer\n",
    "print(output.hidden_states[-3][0]) #10th hidden layer\n",
    "print(output.hidden_states[-4][0]) #9th hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908be4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([12, 768])\n",
      "1 torch.Size([12, 768])\n",
      "2 torch.Size([12, 768])\n",
      "3 torch.Size([12, 768])\n",
      "4 torch.Size([12, 768])\n",
      "5 torch.Size([12, 768])\n",
      "6 torch.Size([12, 768])\n",
      "7 torch.Size([12, 768])\n",
      "8 torch.Size([12, 768])\n",
      "9 torch.Size([12, 768])\n",
      "10 torch.Size([12, 768])\n",
      "11 torch.Size([12, 768])\n",
      "12 torch.Size([12, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(output.hidden_states)):\n",
    "    print(i, output.hidden_states[-1][0].size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
